{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Evaluating the Models "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In notebook **3. Model Building and Hyperparameter Tuning**, I bult a variety of models to classify reddit posts originating from r/depression and r/anxiety subs. The models were\n",
    "\n",
    "    1. Logistic Regression\n",
    "    2. Naive Bayes\n",
    "    3. K Nearest Neighbors \n",
    "    4. Deicsion Tree\n",
    "    5. Bootstrap Aggregating (Bagging)\n",
    "    6. Random Forest\n",
    "    7. Extra Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each model was run in four ways: \n",
    "    1. with default parameters and count vectorizer\n",
    "    2. with count vectorizer and gridsearch \n",
    "    3. with default parameters and tfidf vectorizer\n",
    "    4. with tfidf vectorizer and gridsearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, I have a total of 28 models to be evaluated. For each case I have recorded the cross validation score, the train score and the test score. The following visualization shows the various scores for various models:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Count Vectorizer \n",
    "\n",
    "![count_vec](../images/scores_count_vectorizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Tfidf Vectorizer\n",
    "\n",
    "![count_vec](../images/scores_tfidf_vectorizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models peroform better with Tfidf Vectorizer, and others - with Count Vectorizer. This depends largely on the model and on the data, therefore differences in performance based on the type of vectorizer used cannot be generalized and need to be inspected on a case by case basis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Final Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. The best model - Multinomial Naive Bayes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the evaluation criteria I defined in notebook **1. Problem Statement and Getting Data**, the multinomial naive bayes with tfidf vectorizer is chosen to be the best model for classifying r/depression and r/anxiety posts. This is because naive bayes is the model which best satisfies the two criteria: \n",
    "    1. highest accuracy score \n",
    "AND \n",
    "    2. smallest bias/variance (meaning the difference between train and test scores is the smallest)\n",
    "    \n",
    "    \n",
    "Here is a visualization that showcases the iterative improvements of Naive Bayes: \n",
    "\n",
    "![naive_bayes_scores](../images/naive_bayes_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With thorough hyperparameter tuning thanks to GridSearch, the naive based model has achieved the least difference between train and test scores (~5%). While this difference is not 0, it is the smallest of all the other models tested. So Naive Bayes is the model which has achieved the best balance between variance and bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the best parameters for this model:\n",
    "    \n",
    "1. Best MNB Parameter: alpha = 1.0\n",
    "2. Best TfidfVectorizer() Parameters:  max_df = 0.7,   min_df = 20,   ngram_range = (1,1), stop_words = eng, max_features = 800. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Evaluation metrics of the best model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![eval_metrics_naive_bayes](../images/naive_bayes_eval_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the context of this project, false positive errors are considered equally as bad as false negative errors. Therefore, we can use accuracy as the best evaluation metric to determine how well this model will generalize to out of sample data. The 83% accuracy we achieved beats the baseline accuracy score by about 32%. So we know that the model does have predictive power. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Discussion on Further Improvements\n",
    "\n",
    "There is a variety of things that I have not utilized in building the best model. The use of any such things may help improve the out of sample score of a model and is advised for the next phase of this project. Things that may help improve the classification of subreddit posts: \n",
    "\n",
    "**1. Another model** I used a variety of models but this is not an exhaustive list of all possible models. One of the major problems seen across all models I built was high variance. To combat this issue more advanced ensembling techniques can be used because models with high variance are good candidates for ensembles. One such ensemble model to try next is the **Voting Classifier** which is like a Bagging model but without the restriction of only using decision trees. Voting Classifier can aggregate over all of the models used here, and hopefully, leveraging the 'wisdom of the crowds', can output a model with higher accuracy score.\n",
    "\n",
    "**2. Feature engineering** I have not used lematizing, or stemming or any other tokenizing techniques. Building a custom tokenizer to input into the Counte Vectorizer and/or the Tfidf Vectorizer may help improve the models. \n",
    "\n",
    "**2.1. Removing certain words from stop words** Every model I have built has stop_words removed from its features spaec. The standard english stopwords include pronouns such as \"I, he, she,' etc... However, according to [this article](https://tonic.vice.com/en_us/article/xw58ea/depressed-people-use-these-words-more-often) which summarizes recent research studies on the word choice of poeple who suffer from depression, words like \"I\", \"myself\" and \"me\" are known to be prevalent in the vocabulary of depressed people. Therefore removing these words from the list of stop_words may help the model in better identifying posts coming from r/depressed  and overall improve its predictive performance.\n",
    "\n",
    "**3. Feature selection** It may be worthwhile to do some exploratory analyses into the features in the feature space and remove features that may be adding too much noise to the data. \n",
    "\n",
    "**4. Stacking** Model stacking is another option similar to that of ensembles. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
